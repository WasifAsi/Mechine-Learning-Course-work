{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "df = pd.read_csv('bank.csv')\n",
    "\n",
    "# Inspect the DataFrame to find the correct target column name\n",
    "print(df.columns)\n",
    "\n",
    "# Assuming the target variable is 'y' (adjust if necessary)\n",
    "target_column = 'y'  # Change this to the correct column name if different\n",
    "\n",
    "# Step 2: Handle missing values\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Assuming there are no missing values in this dataset, if there are, you can handle them like this:\n",
    "# df.fillna(df.median(), inplace=True)\n",
    "\n",
    "# Step 3: Encode categorical variables\n",
    "# Identify categorical columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Apply LabelEncoder to each categorical column\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Step 4: Scale numerical features\n",
    "# Identify numerical columns\n",
    "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Apply StandardScaler to numerical columns\n",
    "scaler = StandardScaler()\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Step 5: Separate features and target variable\n",
    "X = df.drop(target_column, axis=1)\n",
    "y = df[target_column]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9055\n",
      "Neural Network Accuracy Pytorch: 0.9077\n",
      "Neural Network Accuracy tensorflow: 0.9008\n",
      "Neural Network performs better.\n"
     ]
    }
   ],
   "source": [
    "# Compare Accuracies\n",
    "rf_accuracy = accuracy_score(y_test, rf_y_pred_test)  # Corrected variable\n",
    "nn_accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"Neural Network Accuracy Pytorch: {nn_accuracy:.4f}\")\n",
    "print(f\"Neural Network Accuracy tensorflow: {nn_ten:.4f}\")\n",
    "\n",
    "if nn_accuracy > rf_accuracy:\n",
    "    print(\"Neural Network performs better.\")\n",
    "elif rf_accuracy > nn_accuracy:\n",
    "    print(\"Random Forest performs better.\")\n",
    "else:\n",
    "    print(\"Both models perform equally well.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'xgboost' has no attribute 'cuda_libs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda_libs\u001b[49m\u001b[38;5;241m.\u001b[39mis_xgboost_gpu_installed():\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU support is available.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     tree_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu_hist\u001b[39m\u001b[38;5;124m'\u001b[39m \n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'xgboost' has no attribute 'cuda_libs'"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "if xgb.cuda_libs.is_xgboost_gpu_installed():\n",
    "    print(\"GPU support is available.\")\n",
    "    tree_method = 'gpu_hist' \n",
    "else:\n",
    "    print(\"GPU support is not available.\")\n",
    "    tree_method = 'hist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'xgboost' has no attribute 'cuda_libs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda_libs\u001b[49m\u001b[38;5;241m.\u001b[39mis_xgboost_gpu_installed():\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU support is available.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     tree_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu_hist\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Use GPU for training\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'xgboost' has no attribute 'cuda_libs'"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "if xgb.cuda_libs.is_xgboost_gpu_installed():\n",
    "    print(\"GPU support is available.\")\n",
    "    tree_method = 'gpu_hist'  # Use GPU for training\n",
    "else:\n",
    "    print(\"GPU support is not available.\")\n",
    "    tree_method = 'hist'      # Use CPU for training\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    tree_method=tree_method,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Utilize all available CPU cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the Neural Network\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    y_pred, y_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Hyperparameter search\n",
    "hidden_sizes = [32, 64, 128]\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [16, 32, 64]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_hyperparameters = {}\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            # Prepare data loaders\n",
    "            train_dataset = TensorDataset(torch.tensor(X_train.values, dtype=torch.float32),\n",
    "                                           torch.tensor(y_train.values, dtype=torch.long))\n",
    "            test_dataset = TensorDataset(torch.tensor(X_test.values, dtype=torch.float32),\n",
    "                                          torch.tensor(y_test.values, dtype=torch.long))\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            # Initialize model, loss, and optimizer\n",
    "            model = NeuralNet(input_size=X_train.shape[1], hidden_size=hidden_size, output_size=2).to(device)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            # Train and evaluate\n",
    "            train_model(model, train_loader, criterion, optimizer, device, epochs=20)\n",
    "            accuracy = evaluate_model(model, test_loader, device)\n",
    "\n",
    "            # Save best hyperparameters\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_hyperparameters = {'hidden_size': hidden_size, 'lr': lr, 'batch_size': batch_size}\n",
    "\n",
    "            print(f\"Hidden Size: {hidden_size}, LR: {lr}, Batch Size: {batch_size}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print(f\"Best Hyperparameters: {best_hyperparameters}\")\n",
    "print(f\"Best Accuracy: {best_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Evaluate Random Forest predictions\n",
    "# rf_y_pred_test = best_rf_model.predict(X_test)\n",
    "# rf_test_accuracy = accuracy_score(y_test, rf_y_pred_test)\n",
    "\n",
    "print(\"Random Forest - Test Accuracy:\", accuracy_score(y_test, rf_y_pred_test))\n",
    "print(\"Random Forest - Classification Report:\")\n",
    "print(classification_report(y_test, rf_y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Neural Network predictions\n",
    "nn_test_accuracy = evaluate_model(model, test_loader, device)\n",
    "print(f\"Neural Network - Test Accuracy: {nn_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# After tuning, you already have the best_model (tuned TensorFlow model)\n",
    "\n",
    "# Predict the outcomes on the test set\n",
    "nn_test_preds_tuned = (best_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate accuracy after tuning for the TensorFlow model\n",
    "test_accuracy = accuracy_score(y_test, nn_test_preds_tuned)\n",
    "\n",
    "# Print the accuracy after tuning\n",
    "print(f\"TensorFlow Neural Network Accuracy (Tuned): {nn_accuracy_tuned:.4f}\")\n",
    "\n",
    "# Optionally, print the classification report for further evaluation\n",
    "print(\"\\nTensorFlow Neural Network Classification Report (Tuned):\")\n",
    "print(classification_report(y_test, nn_test_preds_tuned))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
